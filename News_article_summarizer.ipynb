{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvsEzrSxUfxe",
        "outputId": "0009d3f8-5269-4732-83fd-49d8af7492e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=8fffe17f75f1e6c51b6b4a5233090c015c00df73914660da348350bcab45ec74\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=cbe377b4d4f40c89a8efb316c4315adac00843afc2c66df2ca80a7eae6562ec4\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398379 sha256=072c6349b9de69f41459c9793125128d9390a105089afc1b0ae497c08ac46b0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=4f685d75af434d503b340dac1bfbd18b98e053d4c50b51f7527e57cf0c71526d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk newspaper3k scikit-learn pandas matplotlib wordcloud seaborn networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FUuUsCfbUeis"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from newspaper import Article\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import logging\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yKXFtSlQU1Rk"
      },
      "outputs": [],
      "source": [
        "class NewsArticleSummarizer:\n",
        "     def __init__(self):\n",
        "\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "        nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "     def fetch_article(self, url):\n",
        "\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            return {\n",
        "                'title': article.title,\n",
        "                'text': article.text,\n",
        "                'publish_date': article.publish_date,\n",
        "                'authors': article.authors,\n",
        "                'top_image': article.top_image\n",
        "            }\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error fetching article: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "     def preprocess_text(self, text):\n",
        "\n",
        "\n",
        "        text = text.lower()\n",
        "\n",
        "\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "\n",
        "        processed_words = [\n",
        "            self.lemmatizer.lemmatize(word)\n",
        "            for word in words\n",
        "            if word not in self.stop_words and len(word) > 2\n",
        "        ]\n",
        "\n",
        "        return ' '.join(processed_words)\n",
        "\n",
        "     def generate_summary(self, text, num_sentences=5):\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "\n",
        "        similarity_matrix = self.build_similarity_matrix(sentences)\n",
        "\n",
        "\n",
        "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "        scores = nx.pagerank(nx_graph)\n",
        "\n",
        "\n",
        "        ranked_sentences = sorted(\n",
        "            ((scores[i], sentence) for i, sentence in enumerate(sentences)),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        summary = ' '.join(sentence for _, sentence in ranked_sentences[:num_sentences])\n",
        "        return summary\n",
        "\n",
        "     def build_similarity_matrix(self, sentences):\n",
        "\n",
        "        similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1 != idx2:\n",
        "                    similarity_matrix[idx1][idx2] = self.sentence_similarity(\n",
        "                        sentences[idx1],\n",
        "                        sentences[idx2]\n",
        "                    )\n",
        "\n",
        "        return similarity_matrix\n",
        "\n",
        "     def sentence_similarity(self, sent1, sent2):\n",
        "\n",
        "        words1 = [word.lower() for word in word_tokenize(sent1)]\n",
        "        words2 = [word.lower() for word in word_tokenize(sent2)]\n",
        "\n",
        "        all_words = list(set(words1 + words2))\n",
        "\n",
        "        vector1 = [0] * len(all_words)\n",
        "        vector2 = [0] * len(all_words)\n",
        "\n",
        "        for word in words1:\n",
        "            if word not in self.stop_words:\n",
        "                vector1[all_words.index(word)] += 1\n",
        "\n",
        "        for word in words2:\n",
        "            if word not in self.stop_words:\n",
        "                vector2[all_words.index(word)] += 1\n",
        "\n",
        "        return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "     def extract_keywords(self, text, top_n=10):\n",
        "\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_features=100,\n",
        "            stop_words='english'\n",
        "        )\n",
        "\n",
        "\n",
        "        tfidf_matrix = vectorizer.fit_transform([text])\n",
        "\n",
        "\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        scores = tfidf_matrix.toarray()[0]\n",
        "\n",
        "\n",
        "        keyword_scores = list(zip(feature_names, scores))\n",
        "        keyword_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return keyword_scores[:top_n]\n",
        "\n",
        "     def generate_visualizations(self, text, title):\n",
        "\n",
        "\n",
        "        wordcloud = WordCloud(\n",
        "            width=800,\n",
        "            height=400,\n",
        "            background_color='white'\n",
        "        ).generate(text)\n",
        "\n",
        "\n",
        "        words = word_tokenize(text)\n",
        "        word_freq = Counter(words)\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title('Word Cloud')\n",
        "\n",
        "\n",
        "        plt.subplot(2, 1, 2)\n",
        "        top_words = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10])\n",
        "        sns.barplot(x=list(top_words.values()), y=list(top_words.keys()))\n",
        "        plt.title('Top 10 Most Frequent Words')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{title.replace(' ', '_')}_analysis.png\")\n",
        "        plt.close()\n",
        "\n",
        "     def analyze_article(self, url, summary_sentences=5):\n",
        "        start_time = time.time()\n",
        "        self.logger.info(f\"Starting analysis for URL: {url}\")\n",
        "\n",
        "        article_data = self.fetch_article(url)\n",
        "        if not article_data:\n",
        "            return None\n",
        "\n",
        "\n",
        "        processed_text = self.preprocess_text(article_data['text'])\n",
        "\n",
        "        results = {\n",
        "            'title': article_data['title'],\n",
        "            'original_length': len(article_data['text']),\n",
        "            'summary': self.generate_summary(article_data['text'], summary_sentences),\n",
        "            'keywords': self.extract_keywords(processed_text),\n",
        "            'publish_date': article_data['publish_date'],\n",
        "            'authors': article_data['authors']\n",
        "        }\n",
        "\n",
        "\n",
        "        self.generate_visualizations(processed_text, article_data['title'])\n",
        "\n",
        "        self.logger.info(f\"Analysis completed in {time.time() - start_time:.2f} seconds\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpSRSFgkU143",
        "outputId": "ce0acde2-39e0-4a6c-e639-8aa89122d31d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Article Title: “The Boys” Gets Too Close for Comfort\n",
            "\n",
            "Authors: Inkoo Kang\n",
            "Published: 2024-07-08 00:00:00\n",
            "\n",
            "Original Length: 7641 characters\n",
            "\n",
            "Summary:\n",
            "For Homelander, fatherhood poses fresh, even poignant, challenges. But, perhaps because of the tightrope Kripke has long been able to walk, “The Boys” has been a bona-fide hit for Prime Video, with a final season still to come, two spinoffs (including the teen-oriented “Gen V”), and more in development. In this season, the supe is reunited with Ryan (Cameron Crovetti), the son who’s been kept from him for years—one blessed, and cursed, with supernatural abilities of his own. Yet Homelander, who’s loath to leave him with a “shithole country,” doesn’t know how else to demonstrate his love. As another character puts it, “If Ryan becomes like Homelander, that’s the end of the world.”\n",
            "\n",
            "For a certain subset of viewers, such dire warnings have come as a surprise.\n",
            "\n",
            "Top Keywords:\n",
            "- homelander: 0.4914\n",
            "- season: 0.3024\n",
            "- boy: 0.2268\n",
            "- seven: 0.1890\n",
            "- character: 0.1512\n",
            "- come: 0.1512\n",
            "- kripke: 0.1512\n",
            "- like: 0.1512\n",
            "- people: 0.1512\n",
            "- woman: 0.1512\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    summarizer = NewsArticleSummarizer()\n",
        "    url = \"https://www.newyorker.com/magazine/2024/07/08/the-boys-season-4-review\"\n",
        "\n",
        "    results = summarizer.analyze_article(url)\n",
        "    if results:\n",
        "        print(f\"\\nArticle Title: {results['title']}\")\n",
        "        print(f\"\\nAuthors: {', '.join(results['authors'])}\")\n",
        "        print(f\"Published: {results['publish_date']}\")\n",
        "        print(f\"\\nOriginal Length: {results['original_length']} characters\")\n",
        "        print(\"\\nSummary:\")\n",
        "        print(results['summary'])\n",
        "        print(\"\\nTop Keywords:\")\n",
        "        for keyword, score in results['keywords']:\n",
        "            print(f\"- {keyword}: {score:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aJ53_ZVU1-4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}